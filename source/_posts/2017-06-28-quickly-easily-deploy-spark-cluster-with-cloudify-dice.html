---
layout: blogpost
title: Quickly and easily deploy a Spark cluster with Cloudify and DICE
description: Follow this simple tutorial, by XLab's Matej Artač, to deploy a Spark cluster using Cloudify and the DICE Deployment Service.
image: matej.jpg
author: Matej Artač, Guest Post
tags:
 - Cloud Orchestration
 - Spark
 - TOSCA
 - Big Data
 - Cloudify
---

<notextile>

<img src="/img/blog/cloudify-dice-spark-header.png" alt="Cloudify and DICE orchestrate Spark">
<br/>
<br/>

<p><em>Any technology becomes exciting if one can experience it hands-on. This is a great way to learn to use it, obtain experience, and gradually build useful solutions. In this post, we show how to get started with <a href="https://spark.apache.org/" target="_blank">Apache Spark</a> quickly and easily without losing days of time trying to install it. Read on if you are interested in <a href="/2016/11/10/how-dice-does-hybrid-cloud-orchestration-big-data-using-cloudify-tosca.html" target="_blank">Big Data</a> and have an <a href="/2017/06/07/installing-cloudify-4-aws-openstack-simple-step-by-step-tutorial.html" target="_blank">Amazon AWS</a> account  or an access to an <a href="/2016/10/13/hybrid-cloud-orchestration-on-openstack-with-cloudify-tosca.html" target="_blank">OpenStack</a> cloud.</em></p>
<p dir="auto">In the <a href="http://www.dice-h2020.eu/" target="_blank" rel="nofollow noreferrer">DICE H2020</a> project, we are bringing UML-model-driven development approach into the <a href="/2016/11/02/cloudify-awesome-sauce-devops-monthly-roundup-october-2016.html" target="_blank">DevOps</a> world. Automation of continuous application deployment is an important element, and here <a href="/" target="_blank" rel="nofollow noreferrer">Cloudify</a> has been a great enabler. In the <a href="https://www.dice-h2020.eu/2017/06/19/release-0-3-4-of-dice-deployment-service/" target="_blank" rel="nofollow noreferrer">release 0.3.4</a> of the <a href="https://github.com/dice-project/DICE-Deployment-Service" target="_blank" rel="nofollow noreferrer">DICE components</a>, we are several steps closer to two of our goals: making Continuous Integration and application experimentation easy, and providing building blocks for deploying Big Data applications. In this post we will demonstrate the latter bit on the example of Spark.</p>

<hr>

<span class="pullquote-left">
  <font style="font-weight: bold" size="5" face="Baskerville Old Face"><em>Watch the Big Data for DevOps Webinar!</em></font>&nbsp; <a class="btn btn-medium btn-theme btn-rounded" id="downloadBtnInner" href="/webinars/devops-data-intensive-applications-webinar.html?utm_campaign=DICE%20Webinar&utm_source=website_blogpost_cta&utm_medium=jh" target="_blank"><i class="icon-plus"></i> Go </a></span>
  
<hr>

<p dir="auto">Let us start with some pre-requisites. The recipe we will give you requires an instance of Cloudify Manager 3.4.2 running in the Amazon AWS. We also recommend that you set up the <a href="https://github.com/dice-project/DICE-Deployment-Service" target="_blank" rel="nofollow noreferrer">DICE Deployment Service</a>, a wrapper service for making cloud deployments for DevOps easier. You can follow our <a href="https://github.com/dice-project/DICE-Deployment-Service/blob/master/doc/AdminGuide.md" target="_blank" rel="nofollow noreferrer">instructions</a>, and that will will take about 1 hour for collecting all the configurations and settings. The process of boot-strapping the services is then completely automated, taking about 45 minutes. Now, anyone with an Amazon AWS account can try this out in the EC2.</p>
<p dir="auto">For the rest of the post, we will walk you through the features of a blueprint that we find particularly noteworthy. <a href="https://github.com/dice-project/DICE-Deployment-Examples/blob/master/spark/spark.yaml" target="_blank" rel="nofollow noreferrer">The blueprint</a> deploys a Spark cluster and runs a &pi; calculation on top of it. This is the topology as shown in the Cloudify GUI:</p>
<div class="aligncenter"><img src="/img/blog/cloudify-spark-topology.png" alt="Spark topology" /></a></div>
<br>
<p dir="auto">We start with the header part of the blueprint:</p>

<script src="https://gist.github.com/yeshess/4cc7656c40a8fba9c5e850c5b4c9206e.js"></script>

<p dir="auto">Considering that the DICE TOSCA library is compatible with the Cloudify Manager version 3.4.x, the chosen <a href="/2017/05/23/tosca-times-part-3-model-driven-workflows-cloudify-aria.html" target="_blank">TOSCA</a> dialect is <code>cloudify_dsl_1_3</code>. Then, in the <code>imports</code> we can use just one plug-in to gain the <a href="/2016/12/06/hybrid-vnf-container-orchestration-kubernetes-docker-swarm-using-cloudify.html" target="_blank">multicloud</a> support for the Big Data services.</p>
<p dir="auto">The <code>node_templates</code> section then contains all the needed entities to make a Spark cluster work.</p>

<script src="https://gist.github.com/yeshess/e906db4a135cd97e5e58bae9b52d70f7.js"></script>

<p dir="auto">We start with the master node. In this example, we will make the master node available on a public internet address, thus we add the node template <code>master_ip</code>. We also need to declare a firewall node that we here name <code>master_fw</code> to protect the master host from unsolicited network traffic:</p>

<script src="https://gist.github.com/yeshess/cb3d9725210148d3e6b7ebee875146b9.js"></script>

<p dir="auto">The <code>master_ip</code> is of type <code>dice.VirtualIP</code>, which is an abstraction in the DICE TOSCA library of the concept of what the OpenStack world refer to as the floating IP, and in the AWS it is known as Elastic IP. Similarly, the <code>master_fw</code> is of type <code>dice.firewall_rules.spark.Master</code>. This abstracts the notion of security groups in OpenStack or AWS, which also internally codes in all the rules that are relevant for this particular service type. The designer of the blueprint neither needs to know what they are nor enter them manually into the blueprint.</p>
<p dir="auto">To accommodate for a virtual machine to host the Spark master service, we declare the <code>master_vm</code> node template:</p>

<script src="https://gist.github.com/yeshess/d8df447788e2b387d7cca8f49c66e3ba.js"></script>

<p dir="auto">The type used here is <code>dice.hosts.ubuntu.Medium</code>, which represents a virtual host running the Ubuntu operating system. The host's flavour (i.e., size) is medium. Using the relationships of <code>master_vm</code> we connect the virtual machine with the virtual IP and the firewall node templates.</p>
<p dir="auto">Note that all three of these node templates map to specific concepts handled in the target platform (e.g., OpenStack or Amazon EC2). The plug-in makes sure that the proper API calls get invoked during the orchestration to manipulate the live instances of these constructs.</p>
<p dir="auto">We finish defining the Spark master node with the following node template declaration:</p>

<script src="https://gist.github.com/yeshess/4d2c8d24540b4a749ca0613d5d9fae81.js"></script>

<p dir="auto">The type <code>dice.components.spark.Master</code> unlocks the magic of our TOSCA technology library by running the needed installation and configuration processes on the <code>master_vm</code> instance, installing the Spark master. Again, the author of the blueprint does not have to know what is involved in this installation process. For the curious, however, the <a href="https://github.com/dice-project/DICE-Chef-Repository/tree/master/cookbooks/spark" target="_blank" rel="nofollow noreferrer">Chef cookbook</a> provides the details of this implementation.</p>
<p dir="auto">A Spark cluster also requires a number of Spark worker nodes. We define them in a similar fashion as we did the master node. These nodes don't need any external access, so we skip the virtual IP:</p>

<script src="https://gist.github.com/yeshess/e4cf54b256862b945374e4b12a2d36b1.js"></script>

<p dir="auto">Note that the number assigned to the <code>instances.deploy</code> property can be as high as required by the workload, but for &pi; calculation, 2 is more than enough.</p>

<script src="https://gist.github.com/yeshess/5f6dde63ccc93a6c947293e69f26f942.js"></script>

<p dir="auto">The only difference from the <code>master</code> definition, other than a different type, is that <code>worker</code> is in relationship with the <code>master</code> node template. This ensures that the orchestration will configure the worker with the proper address and port of the master node.</p>
<p dir="auto">The last bit in the node template to be defined is the Spark application. The type to use is <code>dice.components.spark.Application</code>, which defines a submitter of the user's custom application. Here, a number of properties are available, and the blueprint author has to populate them with the values that are specific to the user's application:</p>

<script src="https://gist.github.com/yeshess/92578932b35ffbe2b02445fb6883cab4.js"></script>

<p dir="auto">As the example shows, the properties of the node fully describe the user's application. The <code>jar</code> property points to the <code>.jar</code>-compiled application that is either available online (the orchestrator will fetch it before submitting it to the master) or can be bundled in with the blueprint and referred to locally. The <code>class</code> property defines the class to execute, and the <code>name</code> property reflects in the name of the application when submitted. Finally, <code>args</code> is a free-form list of command line arguments required by the particular application. Here, we only supply one parameter with the value 10.</p>
<p dir="auto">The two relationships are important, because <code>dice.relationships.spark.SubmittedBy</code> declares the master that will both run and receive the application. The <code>dice.relationships.Needs</code>, on the other hand, is present to express a dependency of the client application to the worker nodes. If it was not present, the orchestrator would submit the job to the master before the workers would have been ready, causing delay or possibly even a failure of running the job.</p>
<p dir="auto">The concluding part of the blueprint represents the outputs of the deployment:</p>

<script src="https://gist.github.com/yeshess/2bc163eb6b1c666bb5f1f80fdeec8bb6.js"></script>

<p dir="auto">In this case, we use TOSCA internal function <code>get_attribute</code> to extract a runtime attribute of the floating address to the master service, and <code>concat</code> to compose the final URL.</p>
<p dir="auto">Obtaining a working Spark deployment is then a matter of submitting the blueprint to an instance of DICE Deployment Service. 10-20 minutes later, the cluster will appear and will compute an approximation of the number &pi;. The only disadvantage of the simplicity of the presented example is that this result remains buried in the cluster, so the user has to log into one of the nodes to read it.</p>
<p dir="auto">Now we have a blueprint that we can keep versioned in Git, and use <a href="https://github.com/dice-project/DICE-Jenkins-Plugin" target="_blank" rel="nofollow noreferrer">Continuous Integration</a> to deploy it automatically and frequently. This is perfect for a DevOps approach of developing and validating Big Data applications. In the DICE Deployment Service's GUI, a successfully finished deploy will appear like this:</p>
<div class="aligncenter"><img src="/img/blog/success-spark-deployment-cloudify-dice.png" /></a></div>

</notextile>