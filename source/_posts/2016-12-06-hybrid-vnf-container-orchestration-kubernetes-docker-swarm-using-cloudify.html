---
layout: blogpost
title: Hybrid VNF Container Orchestration With Kubernetes and Docker Swarm Using Cloudify
description: This post will show how Cloudify orchestrates a service chain that spans multiple container management engines such as Kubernetes and Docker Swarm.
image: dewayne.jpg
author: DeWayne Filppi
tags: 
 - NFV
 - Kubernetes
 - Docker Swarm
 - VNF Management
 - Hybrid Cloud
 - Container Orchestration
---

<notextile>

<img src="/img/blog/containerized-hybrid-vnf.png" alt="Kubernetes, Docker, GCP Container Management Engines" width="870">
<br/>
<br/>

<p><b></b>

<p>The need to <a href="{{ site.baseurl }}/network-function-virtualization-vnf-nfv-orchestration-sdn-platform.html" target="_blank">orchestrate virtual network functions</a> (VNFs) that run in Linux hosted containers is an emerging challenge in the <a href="{{ site.baseurl }}/2014/04/19/what-is-nfv-network-function-virtualization-vnf.html" target="_blank">NFV</a> world.  Driven by the need for high responsiveness and deployment density, as well as a desire to adopt modern <a href="{{ site.baseurl }}/tags/Microservices/index.html" target="_blank">microservices</a> architectures, users and vendors are finding containers an appealing prospect.  In this post I address a hybrid <a href="{{ site.baseurl }}/2016/11/17/open-source-orchestration-management-complex-vnfs-using-openstack-controller-edd-devices.html" target="_blank">VNF orchestration</a> consisting of Kubernetes and Swarm container managers, as well as conventional hosts.  The idea is to explore the challenges and opportunities presented by such a scenario.  The world of containerized VNFs is somewhat limited (to say the least) that will run out of the box in Kubernetes and/or Docker Swarm, so I did this exploration with containerized versions of standard Linux network components, the Quagga router and Nginx configured as a load balancer.</p>

<hr>

<span class="pullquote-left">
  <font style="font-weight: bold" size="5" face="Baskerville Old Face"><em>Join Our Cloud and DevOps Orchestration Webinars Today!</em></font>&nbsp; <a class="btn btn-medium btn-theme btn-rounded" id="downloadBtnInner" href="{{ site.baseurl }}/webinars.html" target="_blank"><i class="icon-plus"></i> Go </a></span>
  
  <hr>

<h2>
<a id="user-content-target-architecture" class="anchor" href="#target-architecture" aria-hidden="true"></a>Target Architecture</h2>

<p class="aligncenter"><img src="/img/blog/nfv-container-architecture.png" alt="Architecture" width="600"></p>

<p>The architecture is composed of some familiar building blocks; the <a href="https://github.com/cloudify-examples/kubernetes-cluster-blueprint">Kubernetes blueprint</a>, the <a href="https://github.com/cloudify-examples/cloudify-kubernetes-plugin">Kubernetes plugin</a>, the <a href="https://github.com/cloudify-examples/docker-swarm-blueprint">Docker Swarm Blueprint</a>, and the <a href="https://github.com/cloudify-examples/cloudify-proxy-plugin">Deployment Proxy plugin</a>.  In addition, a yet to be published Docker Swarm plugin is used.</p>

<p>The basic idea is to run network traffic through one VNF on <a href="{{ site.baseurl }}/2016/09/21/orchestrating-docker-swarm-with-cloudify.html" target="_blank">Docker Swarm</a>, then through another VNF in <a href="{{ site.baseurl }}/2016/07/13/cloudify-and-kubernetes-cluster-hybrid-stack-orchestration-cloud-deployment-automation.html" target="_blank">Kubernetes</a>.  In this example, traffic is load balanced though an Nginx container on Swarm, which load balances a couple of VMs on the other side of a containerized Quagga instance.</p>

<p class="aligncenter"><img src="/img/blog/packet-flow.png" alt="Packet Flow" width="600"></p>

<h2>
<a id="user-content-automation-process" class="anchor" href="#automation-process" aria-hidden="true"></a>Automation Process</h2>

<p>The first step in any automated orchestration is to identify (and verify when needed) the steps to create the desired end state.  The end state for Cloudify is defined by a TOSCA blueprint or blueprints.  Clearly a Kubernetes and Swarm cluster are needed, along with the requisite networking setup.  To reflect a reasonable production setup, these clusters should have separate deployment lifecycles, and therefore be modeled as separate blueprints.  The existing Kubernetes and Swarm cluster blueprints are a good starting point, but need some tweaking.  Then a third blueprint is needed to actually deploy and configure the containers on each of the clusters.</p>

<h2>
<a id="user-content-kubernetes-cluster-blueprint" class="anchor" href="#kubernetes-cluster-blueprint" aria-hidden="true"></a>Kubernetes Cluster Blueprint</h2>

<p>Whenever considering an orchestration plan that consists of multiple blueprints, a key factor to consider is the outputs.  The <a href="{{ site.baseurl }}" target="_blank">Cloudify</a> deployment proxy achieves its aims by copying outputs from configured blueprints, so the containing blueprint can perform the tasks it needs.  In this case, the "containing blueprint" will be the blueprint that deploys the services onto the Kubernetes cluster and (potentially) gets the IP addresses of the target VMs running Apache.  The Kubernetes cluster URL is already in the outputs of the standard Kubernetes blueprint.  For convenience, the Kubernetes blueprint will be changed to create the <code>10.100.102.0/24</code> network and the Apache VMs.  By virtue of starting the VMs, the blueprint will have access to the IPs and of course the subnet (by virtue of a Cloud API, a host IP pool, or hard coding).  These can be then exposed in the outputs like so:</p>

<div class="highlight highlight-source-yaml"><pre><span class="pl-ent">outputs:</span>
  <span class="pl-ent">apache_info:</span>
    <span class="pl-ent">value:</span>
      <span class="pl-ent">network:</span> <span class="pl-s">{get_property: [ apache_subnet, subnet, cidr ]</span>
      <span class="pl-ent">ips:</span> <span class="pl-s">{concat: [ {get_attribute: [ apache_host1, ip ] }, "," , {get_attribute: [ apache_host2, ip ] } ] }</span></pre></div>

<p>In addition to simply starting the instance, the Apache web server is started and supplied with an identifyable <code>index.html</code> so that load balancing can be verified via <code>curl</code>.  Also, to match the architecture, the Kubernetes blueprint must be changed to run the cluster in the <code>10.100.101.0/24</code> network.  To simplify the setup, the cluster will only have a single node that will contain the Quagga router, and have an interface to the <code>10.100.102.0/24</code> network.</p>

<h2>
<a id="user-content-vnf-preparation" class="anchor" href="#vnf-preparation" aria-hidden="true"></a>VNF Preparation</h2>

<p>Quagga operates by manipulating the Linux kernel routing tables.  Unprivileged containers run in their own network namespace, and so won't affect the default tables.  To allow Quagga to access the routing tables, it must run in privileged mode, which is enabled by running the Kubernetes daemon (kubelet) with the <code>--allow-privileged</code> option.  The example Kubernetes blueprint already does this.  In addition to privileged mode is providing access to the host network stack.  This is covered in the section about the service blueprint.</p>

<h2>
<a id="user-content-docker-swarm-blueprint" class="anchor" href="#docker-swarm-blueprint" aria-hidden="true"></a>Docker Swarm Blueprint</h2>

<p>The existing Docker Swarm cluster blueprint remains mostly unchanged except for locating the cluster in the <code>10.100.100.0/24</code> network, as well as having an interface to the <code>10.100.101.0/24</code> network.  In the case of an <a href="{{ site.baseurl }}/2014/07/10/what-is-openstack-tutorial.html" target="_blank">Openstack</a> platform, this would mean defining a Port node on the existing Kubernetes network, and defining a relationship between the instance and port.</p>

<p>As we anticipate the final configuration, it should be noted that there is no routing rule on the Swarm hosts to send traffic bound for 1.100.102/24 to Quagga for routing.  There are a couple ways of handling this statically, but maybe the simplest is just adding a <code>userdata</code> section to add the route (assuming cloud infrastructure).  A bare metal setup might just use the Fabric plugin to add the rule remotely.  The <code>userdata</code> looks (paraphrasing) like:</p>

<div class="highlight highlight-source-yaml"><pre><span class="pl-ent">userdata:</span> <span class="pl-s">|</span>
<span class="pl-s">  ip route add { get_input: [ quagga_net ] } via { get_input: [ quagga_host ] } dev eth1</span></pre></div>

<h2>
<a id="user-content-service-blueprint" class="anchor" href="#service-blueprint" aria-hidden="true"></a>Service Blueprint</h2>

<p>The <code>service</code> blueprint has the responsibility to deploy the microservices (i.e. VNFs) to both clusters and configuring them properly.  It does this by exploiting a plugin that "proxies" the Kubernetes and Swarm blueprints as described earlier, and by using the Kubernetes and Swarm plugins to do the actual deployment.</p>

<h2>
<a id="user-content-orchestrating-the-quagga-container-on-kubernetes" class="anchor" href="#orchestrating-the-quagga-container-on-kubernetes" aria-hidden="true"></a>Orchestrating The Quagga Container on Kubernetes</h2>

<p>Quagga is deployed on Kubernetes using a native Kubernetes descriptor.  For this example Quagga was only deployed to serve simple static routes.  As is typical with the Kubernetes plugin, a Kubernetes descriptor is referred to in the blueprint possibly with some overrides and environment variables that the container(s) can use to self configure.  In this case, the Quagga router is seeded with some static routes created by examining the outputs of the deployment proxy for the Kubernetes deployment, and passing them in the environment to the container.</p>

<div class="highlight highlight-source-yaml"><pre>  <span class="pl-ent">quagga:</span>
    <span class="pl-ent">type:</span> <span class="pl-s">cloudify.kubernetes.Microservice</span>
    <span class="pl-ent">properties:</span>
      <span class="pl-ent">name:</span> <span class="pl-s">nginx</span>
      <span class="pl-ent">ssh_username:</span> <span class="pl-s">ubuntu</span>
      <span class="pl-ent">ssh_keyfilename:</span> <span class="pl-s">/root/.ssh/agent_key.pem</span>
      <span class="pl-ent">config_files:</span>
        - <span class="pl-ent">file:</span> <span class="pl-s">resources/kubernetes/pod.yaml</span>
        - <span class="pl-ent">file:</span> <span class="pl-s">resources/kubernetes/service.yaml</span>
      <span class="pl-ent">env:</span>
        <span class="pl-ent">ROUTES:</span> <span class="pl-s">[ {concat: [ get_property: [ kubernetes_proxy, vm_info, apache_subnet ], " dev eth1" ]} ]</span>
    <span class="pl-ent">relationships:</span>
      - <span class="pl-ent">type:</span> <span class="pl-s">cloudify.kubernetes.relationships.connected_to_master</span>
        <span class="pl-ent">target:</span> <span class="pl-s">kubernetes_proxy</span></pre></div>

<p>The Quagga container deployment descriptor is simple, but note that it must be run with privileged access and use the host network stack:</p>

<div class="highlight highlight-source-yaml"><pre><span class="pl-ent">apiVersion:</span> <span class="pl-s">v1</span>
<span class="pl-ent">kind:</span> <span class="pl-s">ReplicationController</span>
<span class="pl-ent">metadata:</span>
  <span class="pl-ent">name:</span> <span class="pl-s">quagga</span>
<span class="pl-ent">spec:</span>
  <span class="pl-ent">replicas:</span> <span class="pl-c1">1</span>
  <span class="pl-ent">selector:</span>
    <span class="pl-ent">app:</span> <span class="pl-s">quagga</span>
  <span class="pl-ent">template:</span>
    <span class="pl-ent">metadata:</span>
      <span class="pl-ent">name:</span> <span class="pl-s">quagga</span>
      <span class="pl-ent">labels:</span>
        <span class="pl-ent">app:</span> <span class="pl-s">quagga</span>
    <span class="pl-ent">spec:</span>
      <span class="pl-ent">hostNetwork:</span> <span class="pl-c1">true</span>
      <span class="pl-ent">containers:</span>
      - <span class="pl-ent">name:</span> <span class="pl-s">quagga</span>
        <span class="pl-ent">image:</span> <span class="pl-s">dfilppi/quagga</span>
        <span class="pl-ent">workingDir:</span> <span class="pl-s">/</span>
        <span class="pl-ent">command:</span> <span class="pl-s">["bash","start.sh"]</span>
        <span class="pl-ent">ports:</span>
        - <span class="pl-ent">containerPort:</span> <span class="pl-c1">2601</span>
          <span class="pl-ent">hostIP:</span> <span class="pl-s">0.0.0.0</span>
        <span class="pl-ent">securityContext:</span>
          <span class="pl-ent">_privileged:</span> <span class="pl-s">true_</span></pre></div>

<p>The start script in the container takes care of populating the static routes and starting the router.  As a side note, it is assumed that ip forwarding is turned in the instance.</p>

<h2>
<a id="user-content-deploying-and-configuring-the-nginx-container" class="anchor" href="#deploying-and-configuring-the-nginx-container" aria-hidden="true"></a>Deploying and Configuring The Nginx Container</h2>

<p>The last piece of the puzzle is deploying the Nginx container.  The only significant configuration step is populating the load balance host list.  The approach is similar to that used in Kubernetes (passing config in environment variables), but the plugin is different.  Whereas the Kubernetes plugin uses native Kubernetes descriptors, the current version of the Swarm plugin does not handle the equivalent for Swarm (Docker Compose).  Instead, the configuration in the blueprint is more explicit, with the plugin defining <code>cloudify.swarm.Microservice</code>,<code>cloudify.swarm.Container</code>, and <code>cloudify.swarm.Port</code> types.  The microservice is loaded into the defined container, and the ports are exposed via relationships.  In this case, the container accepts the environment config and image reference.</p>

<div class="highlight highlight-source-yaml"><pre>  <span class="pl-ent">nginx_container:</span>
    <span class="pl-ent">type:</span> <span class="pl-s">cloudify.swarm.Container</span>
    <span class="pl-ent">properties:</span>
      <span class="pl-ent">image:</span> <span class="pl-s">dfilppi/nginx2</span>
      <span class="pl-ent">entry_point:</span> <span class="pl-s">start.sh</span>
      <span class="pl-ent">env:</span>
        <span class="pl-ent">SERVERS:</span> <span class="pl-s">{get_attribute: [kubernetes_proxy,vm_info,ips]}</span></pre></div>

<p>Note how the <code>SERVERS</code> definition connects the dynamic outputs of the Kubernetes blueprint to the container configuration in the Swarm cluster.  That pattern of proxied, hybrid orchestration has application far from this esoteric use case.  It is similar to the approach taken in a previous orchestration that demonstrated scaling in Kubernetes triggered by activity on cloud <a href="{{ site.baseurl }}/2016/03/24/openstack-scaling-kubernetes-microservices-linux-containers-cloud-TOSCA-orchestration.html">VMs</a>.</p>

<p>You can watch the video demo of this use case below:</p>

<div class="flexslider aligncenter">
    <ul class="slides">
      <li style="display: list-item;">
        <a class="hover-wrap fancybox fancybox.iframe" data-fancybox-group="gallery" title="Hybrid VNF Container Orchestration With Kubernetes and Docker Swarm Using Cloudify" href="https://www.youtube.com/embed/QeBN1uqyDho?enablejsapi=1&amp;wmode=opaque">
        <br> <br>
        <img src="/img/hybridvnfthumbnail.png" alt="Hybrid VNF Container Orchestration With Kubernetes and Docker Swarm Using Cloudify"></a>
      </li>
    </ul>
  </div>


<h2>
<a id="user-content-conclusion" class="anchor" href="#conclusion" aria-hidden="true"></a>Conclusion</h2>

<p>This post demonstrated Cloudify orchestrating a service chain that spans multiple container management engines.  The reason is to show the flexibility of Cloudify in a "no-standard" world, where VNFs may be opinionated about the manager they run on.  In such a world, Cloudify can be used to orchestrate diverse platforms for the purpose of delivering end user solutions regardless of the Next Big Thing that may come along. The same principles can be applied to other container and IAAS orchestrators (e.g. Swarm, Mesos, Openstack Heat, AWS CloudFormation).  It also permits orchestration of legacy systems and hardware as needed, using a standard open model (TOSCA).  The unopinionated nature of Cloudify also allows the targeting of high performance/bare metal platforms, which is critical for NFV in particular.  Source code for this example is forthcoming. </p>


</notextile>
